{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac8eb95",
   "metadata": {},
   "source": [
    "Let's start by creating a mapping of gene oncology (GO) term IDS to their label indices starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2190f932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO:0000001 => 0\n",
      "GO:0000002 => 1\n",
      "GO:0000003 => 2\n",
      "GO:0000011 => 3\n",
      "GO:0000012 => 4\n",
      "GO:0000017 => 5\n",
      "GO:0000018 => 6\n",
      "GO:0000019 => 7\n",
      "GO:0000020 => 8\n",
      "GO:0000022 => 9\n",
      "\n",
      "\n",
      "GO:0000015 => 0\n",
      "GO:0000108 => 1\n",
      "GO:0000109 => 2\n",
      "GO:0000110 => 3\n",
      "GO:0000111 => 4\n",
      "GO:0000112 => 5\n",
      "GO:0000113 => 6\n",
      "GO:0000118 => 7\n",
      "GO:0000120 => 8\n",
      "GO:0000123 => 9\n",
      "\n",
      "\n",
      "GO:0000005 => 0\n",
      "GO:0000006 => 1\n",
      "GO:0000007 => 2\n",
      "GO:0000008 => 3\n",
      "GO:0000009 => 4\n",
      "GO:0000010 => 5\n",
      "GO:0000014 => 6\n",
      "GO:0000016 => 7\n",
      "GO:0000026 => 8\n",
      "GO:0000030 => 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "obo_file_path = \"./dataset/train/go-basic.obo\"\n",
    "\n",
    "bp_terms, cc_terms, mf_terms = [], [], []\n",
    "\n",
    "with open(obo_file_path, 'r') as file:\n",
    "    data = file.read().split(\"[Term]\")\n",
    "\n",
    "for term_data in data[1:]:  # Skip the first element as it is before the first [Term].\n",
    "    lines = term_data.strip().splitlines()\n",
    "\n",
    "    term_id = None\n",
    "    namespace = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"id:\"):\n",
    "            term_id = line.split(\"id:\")[1].strip()\n",
    "        elif line.startswith(\"namespace:\"):\n",
    "            namespace = line.split(\"namespace:\")[1].strip()\n",
    "\n",
    "        if term_id and namespace:\n",
    "            match namespace:\n",
    "                case \"biological_process\":\n",
    "                    bp_terms.append(term_id)\n",
    "                case \"cellular_component\":\n",
    "                    cc_terms.append(term_id)\n",
    "                case \"molecular_function\":\n",
    "                    mf_terms.append(term_id)\n",
    "                case _:\n",
    "                    # Ignore other namespaces\n",
    "                    pass\n",
    "            break\n",
    "\n",
    "# Print the first 10 terms.\n",
    "for term_mapping in [bp_terms, cc_terms, mf_terms]:\n",
    "    for label_index, term_id in enumerate(term_mapping[:10]):\n",
    "        print(f\"{term_id} => {label_index}\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b390991",
   "metadata": {},
   "source": [
    "Next, let's count the number of unique GO terms so we know how many classes we need to represent in the output layer of the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5891b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biological process classes: 30,510\n",
      "Number of cellular component classes: 4,469\n",
      "Number of molecular function classes: 12,438\n",
      "\n",
      "\n",
      "Total number of classes: 47,417\n"
     ]
    }
   ],
   "source": [
    "num_bp_classes = len(bp_terms)\n",
    "num_cc_classes = len(cc_terms)\n",
    "num_mf_classes = len(mf_terms)\n",
    "\n",
    "total_num_classes = num_bp_classes + num_cc_classes + num_mf_classes\n",
    "\n",
    "print(f\"Number of biological process classes: {num_bp_classes:,}\")\n",
    "print(f\"Number of cellular component classes: {num_cc_classes:,}\")\n",
    "print(f\"Number of molecular function classes: {num_mf_classes:,}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Total number of classes: {total_num_classes:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9310c1",
   "metadata": {},
   "source": [
    "Now we can map the sequence IDs to their GO term label indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a24a6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A009IHW8 => [3525, 9994, 20271, 12676, 2711, 24639, 12657, 24703, 3524, 24891, 24892, 3712, 2161, 12683, 14137, 6596, 14125, 16750, 12664, 6668, 6855, 2758, 6891, 24704, 2180, 14270, 4267, 2687, 2747, 9984, 20269, 19639, 6598, 24901, 20274, 14079, 3758, 20272, 2212, 3653, 12658, 2744, 6888]\n",
      "A0A021WW32 => [15123, 15111, 7537, 18570, 3160, 2966, 22063, 11798, 14805, 3524, 26195, 3270, 4744, 15286, 14770, 6468, 7709, 119, 14771, 303, 7570, 14996, 22909, 2957, 14964, 4207, 5776, 8792, 16992, 4267, 5639, 19713, 2968, 4767, 7233, 2970, 14627, 2914, 31, 117, 7723, 8791, 4643, 14932, 18664, 15689, 19630]\n",
      "A0A023FFD0 => [17038, 4262, 18570, 4787, 17036, 7646, 3524, 29398, 4644, 15541, 15399, 15286, 4785, 15397, 12607, 23519, 15290, 4260, 14860, 11436, 8530, 7640, 18572, 23535, 15539, 7806, 12263, 14810, 23534, 14806, 14862, 23518, 11435, 29399, 8531, 7808, 8521]\n",
      "A0A023GPJ3 => [14392, 18571, 7640, 18570, 1424, 4787, 7646, 26616, 15501, 15504, 15506, 3524, 21242, 1423, 4945, 21064, 15286, 4785, 14810, 14806, 21244, 9069, 15503, 26617, 14387, 15290]\n",
      "A0A023GPK8 => [21461, 10061, 526, 11426, 21146, 8792, 15289, 10060, 15111, 18570, 3160, 11652, 4015, 4201, 3524, 14867, 3598, 476, 1575, 29281, 15286, 15133, 15011, 8791, 14996, 3260, 14800, 23277, 3291, 7588]\n",
      "A0A023GQ97 => [19455, 16028, 3152, 18571, 4267, 3990, 7641, 18570, 3915, 5651, 3488, 22285, 3486, 3524, 22439, 19577, 22116, 15374, 19461, 10240, 3888, 4004, 3048, 8791, 11693, 19205, 3464, 3153, 19459, 22286]\n",
      "A0A023GRW4 => [7666, 9163, 3971, 11434, 4261, 14933, 15123, 15111, 7645, 3160, 3498, 4015, 14805, 7669, 11597, 13141, 3057, 15286, 14770, 14861, 11647, 7709, 21808, 10109, 9164, 14996, 4260, 15068, 19713, 20163, 3283, 22736, 22735, 18216, 10111, 8791, 14932, 3501, 306, 7537, 18570, 308, 3279, 3524, 3282, 3270, 30452, 15374, 4785, 8205, 30454, 15290, 14964, 14860, 17933, 3497, 8792, 4267, 5639, 7640, 4786, 13139, 2868, 15113, 3278, 7723, 14809]\n",
      "A0A023GU64 => [19093, 15111, 14968, 12063, 3160, 4015, 4201, 14970, 14845, 7669, 3524, 12601, 14871, 420, 14839, 14969, 22730, 14996, 14800, 7670, 8792, 14966, 4267, 5639, 4126, 19713, 22735, 12952, 17008, 2914, 27849, 8791, 4128, 14971, 7592]\n",
      "A0A023GU65 => [19093, 15111, 14968, 12063, 3160, 4015, 4201, 14970, 14845, 7669, 3524, 12601, 14871, 420, 14839, 14969, 22730, 14996, 14800, 7670, 8792, 14966, 4267, 5639, 4126, 19713, 22735, 12952, 17008, 2914, 27849, 8791, 4128, 14971, 7592]\n",
      "A0A023GUT0 => [4207, 11644, 16992, 15666, 8247, 18570, 661, 13610, 15246, 14805, 659, 3524, 20810, 15247, 4707, 6466, 4744, 15286, 15665, 8249, 6468, 15604, 8291, 8289, 15607, 11646, 11782, 11781, 15290, 15606, 14809]\n",
      "\n",
      "\n",
      "A0A021WW32 => [3737, 40, 280, 2113, 108, 111, 105, 2118, 1562, 2116, 2119, 291, 354, 2115, 1444, 109, 3597, 2120, 349, 236, 2403, 344, 110, 609, 3843, 365, 2705, 2114, 99, 1438]\n",
      "A0A021WZA4 => [3117, 236, 3843, 845, 511]\n",
      "A0A023GPJ3 => [381, 466, 236, 280, 3843]\n",
      "A0A023GUT0 => [3843, 236, 237, 274]\n",
      "A0A023IM54 => [2008, 280, 1210, 2113, 3953, 423, 236, 845, 2118, 1562, 429, 2076, 2116, 3843, 381, 2114, 3612, 812, 1447, 3623]\n",
      "A0A023PZB3 => [280, 2116, 2113, 3843, 382, 381, 236, 2114, 2118]\n",
      "A0A023T787 => [308, 280, 1444, 2116, 2113, 3843, 2705, 896, 2120, 236, 2114, 1438, 893, 2118, 291]\n",
      "A0A023VTS2 => [280, 2116, 2113, 3843, 236, 2114, 291, 2118]\n",
      "A0A024A2C9 => [3843, 236, 786]\n",
      "A0A024CBD6 => [3843, 236, 845]\n",
      "\n",
      "\n",
      "A0A009IHW8 => [399, 626, 3748, 3758, 3757, 513]\n",
      "A0A023FBW4 => [4619, 1859, 399, 4618, 4621, 1876]\n",
      "A0A023FBW7 => [4620, 4619, 1859, 399, 4618, 1876]\n",
      "A0A023FDY8 => [4620, 4619, 1859, 399, 4618, 1876]\n",
      "A0A023FF81 => [4619, 1859, 399, 4618, 4621, 1876]\n",
      "A0A023FFB5 => [4620, 4619, 1859, 399, 4618, 1876]\n",
      "A0A023FFD0 => [4620, 4619, 1859, 399, 4618, 1876]\n",
      "A0A023FT45 => [4620, 4619, 1859, 399, 4618, 1876]\n",
      "A0A023G6B6 => [4620, 4619, 1859, 399, 4618, 1876]\n",
      "A0A023G9N9 => [4620, 4619, 1859, 399, 4618, 1876]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "terms_path = \"./dataset/train/train_terms.tsv\"\n",
    "\n",
    "# Convert to dict for O(1) label index lookup.\n",
    "bp_terms = {term: index for index, term in enumerate(bp_terms)}\n",
    "cc_terms = {term: index for index, term in enumerate(cc_terms)}\n",
    "mf_terms = {term: index for index, term in enumerate(mf_terms)}\n",
    "\n",
    "bp_sequence_to_label_indices = defaultdict(list)\n",
    "cc_sequence_to_label_indices = defaultdict(list)\n",
    "mf_sequence_to_label_indices = defaultdict(list)\n",
    "\n",
    "df = pd.read_csv(terms_path, sep='\\t')\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sequence_id = row[\"EntryID\"]\n",
    "    aspect = row[\"aspect\"]\n",
    "    term_id = row[\"term\"]\n",
    "\n",
    "    match aspect:\n",
    "        case \"BPO\":\n",
    "            label_index = bp_terms[term_id]\n",
    "            bp_sequence_to_label_indices[sequence_id].append(label_index)\n",
    "        case \"CCO\":\n",
    "            label_index = cc_terms[term_id]\n",
    "            cc_sequence_to_label_indices[sequence_id].append(label_index)\n",
    "        case \"MFO\":\n",
    "            label_index = mf_terms[term_id]\n",
    "            mf_sequence_to_label_indices[sequence_id].append(label_index)\n",
    "        case _:\n",
    "            # Ignore other aspects\n",
    "            continue\n",
    "\n",
    "bp_first_10 = dict(islice(bp_sequence_to_label_indices.items(), 10))\n",
    "cc_first_10 = dict(islice(cc_sequence_to_label_indices.items(), 10))\n",
    "mf_first_10 = dict(islice(mf_sequence_to_label_indices.items(), 10))\n",
    "\n",
    "# Print the first 10 sequence to label index mappings.\n",
    "for sequence_to_label_mapping in [bp_first_10, cc_first_10, mf_first_10]:\n",
    "    for sequence_id, label_indices in sequence_to_label_mapping.items():\n",
    "        print(f\"{sequence_id} => {label_indices}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c1560",
   "metadata": {},
   "source": [
    "Let's read through the training samples and create a new JSONL dataset that contains the raw protein sequences and a list of their GO term label indices for each of the GO subgraphs. This will be the file that we read from when fine-tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57ceade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./dataset/mf_dataset.jsonl\n",
      "Dataset saved to ./dataset/bp_dataset.jsonl\n",
      "Dataset saved to ./dataset/cc_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "fasta_path = \"./dataset/train/train_sequences.fasta\"\n",
    "\n",
    "mf_dataset_path = \"./dataset/mf_dataset.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp_dataset.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc_dataset.jsonl\"\n",
    "\n",
    "for dataset_path, sequence_to_label_indices in [\n",
    "    (mf_dataset_path, mf_sequence_to_label_indices),\n",
    "    (bp_dataset_path, bp_sequence_to_label_indices),\n",
    "    (cc_dataset_path, cc_sequence_to_label_indices),\n",
    "]:\n",
    "    with open(dataset_path, \"w\") as dataset_file:   \n",
    "        with open(fasta_path, \"r\") as fasta_file:\n",
    "            for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                sequence_id = record.id\n",
    "                sequence = str(record.seq)\n",
    "\n",
    "                label_indices = sequence_to_label_indices[sequence_id]\n",
    "\n",
    "                if len(label_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                line = {\n",
    "                    \"sequence_id\": sequence_id,\n",
    "                    \"sequence\": sequence,\n",
    "                    \"label_indices\": label_indices\n",
    "                }\n",
    "\n",
    "                dataset_file.write(json.dumps(line) + \"\\n\")\n",
    "\n",
    "            print(f\"Dataset saved to {dataset_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
