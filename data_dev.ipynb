{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import EsmTokenizer\n",
    "\n",
    "\n",
    "class CAFA5(Dataset):\n",
    "    \"\"\"\n",
    "    The CAFA5 dataset is a collection of protein sequences and their associated gene oncology terms.\n",
    "    It is used for training and evaluating models for protein function prediction.\n",
    "\n",
    "    The dataset is divided into three subsets based on the type of gene ontology terms:\n",
    "    1. Molecular Function (MF)\n",
    "    2. Cellular Component (CC)\n",
    "    3. Biological Process (BP)\n",
    "    \"\"\"\n",
    "\n",
    "    DATASET_NAME = \"andrewdalpino/CAFA5\"\n",
    "\n",
    "    AVAILABLE_SUBSETS = {\"all\", \"mf\", \"cc\", \"bp\"}\n",
    "\n",
    "    AVAILABLE_SPLITS = {\"train\", \"test\"}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        subset: str,\n",
    "        split: str,\n",
    "        tokenizer: EsmTokenizer,\n",
    "        context_length: int,\n",
    "        filter_long_sequences: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if subset not in self.AVAILABLE_SUBSETS:\n",
    "            raise ValueError(f\"Subset '{subset}' is invalid.\")\n",
    "\n",
    "        if split not in self.AVAILABLE_SPLITS:\n",
    "            raise ValueError(f\"Split '{split}' is invalid.\")\n",
    "\n",
    "        if context_length < 1:\n",
    "            raise ValueError(\n",
    "                f\"Context length must be greater than 0, {context_length} given.\"\n",
    "            )\n",
    "\n",
    "        dataset = load_dataset(self.DATASET_NAME, subset)\n",
    "\n",
    "        if filter_long_sequences:\n",
    "            dataset = dataset.filter(\n",
    "                lambda sample: sample[\"length\"] <= context_length - 2\n",
    "            )\n",
    "\n",
    "        terms_to_label_indices = {}\n",
    "\n",
    "        label_index = 0\n",
    "\n",
    "        for subset in dataset.values():\n",
    "            for sample in subset:\n",
    "                for term in sample[\"terms\"]:\n",
    "                    if term not in terms_to_label_indices:\n",
    "                        terms_to_label_indices[term] = label_index\n",
    "\n",
    "                        label_index += 1\n",
    "\n",
    "        num_classes = len(terms_to_label_indices)\n",
    "\n",
    "        dataset = dataset[split]\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.terms_to_label_indices = terms_to_label_indices\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    @property\n",
    "    def label_indices_to_terms(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary mapping label indices to their corresponding gene ontology terms.\n",
    "        \"\"\"\n",
    "\n",
    "        return {index: term for term, index in self.terms_to_label_indices.items()}\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        sample = self.dataset[index]\n",
    "\n",
    "        out = self.tokenizer(\n",
    "            sample[\"sequence\"],\n",
    "            padding=\"max_length\",\n",
    "            padding_side=\"right\",\n",
    "            max_length=self.context_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        attn_mask = out[\"attention_mask\"]\n",
    "        tokens = out[\"input_ids\"]\n",
    "\n",
    "        labels = [0.0] * self.num_classes\n",
    "\n",
    "        for term in sample[\"terms\"]:\n",
    "            label_index = self.terms_to_label_indices[term]\n",
    "\n",
    "            labels[label_index] = 1.0\n",
    "\n",
    "        attn_mask = torch.tensor(attn_mask, dtype=torch.int64)\n",
    "\n",
    "        x = torch.tensor(tokens, dtype=torch.int64)\n",
    "        y = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        assert attn_mask.size(0) == self.context_length\n",
    "\n",
    "        assert x.size(0) == self.context_length\n",
    "        assert y.size(0) == self.num_classes\n",
    "\n",
    "        return x, y, attn_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
